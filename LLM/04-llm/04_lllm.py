import os
from dotenv import load_dotenv
import pandas as pd
from typing import List
from langchain.schema.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.schema.messages import HumanMessage, SystemMessage

# --- 1. YapÄ±landÄ±rma ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise EnvironmentError("OpenAI API anahtarÄ± .env dosyasÄ±nda bulunamadÄ±.")

# --- 2. Sabitler ---
EXCEL_DOSYA_YOLU = r"C:\Users\fb\Desktop\KAÄ°RU\LLM-portfolio\04-llm\04-LLM.xlsx"
SORU_SUTUNU_ADI = "Soru"
CEVAP_SUTUNU_ADI = "Cevap"

# --- 3. Veri YÃ¼kleme ve Ä°ÅŸleme ---
def load_and_chunk_data(file_path: str) -> List[Document]:
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Dosya bulunamadÄ±: {file_path}")
    df = pd.read_excel(file_path, engine='openpyxl').dropna(subset=[SORU_SUTUNU_ADI, CEVAP_SUTUNU_ADI])
    print(f"'{os.path.basename(file_path)}' yÃ¼klendi, {len(df)} geÃ§erli satÄ±r bulundu.")
    splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=30)
    all_chunks = []
    for _, row in df.iterrows():
        combined_text = f"Soru: {row[SORU_SUTUNU_ADI]}\nCevap: {row[CEVAP_SUTUNU_ADI]}"
        chunks = splitter.split_text(combined_text)
        for i, chunk_text in enumerate(chunks):
            all_chunks.append(Document(page_content=chunk_text, metadata={'original_question': row[SORU_SUTUNU_ADI]}))
    print(f"Toplam {len(all_chunks)} adet chunk oluÅŸturuldu.")
    return all_chunks

# --- 4. Pipeline Kurulumu ---
all_chunks = load_and_chunk_data(EXCEL_DOSYA_YOLU)
embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)
print("VektÃ¶r veritabanÄ± oluÅŸturuluyor...")
db = FAISS.from_documents(all_chunks, embedding_model)
print("VektÃ¶r veritabanÄ± (FAISS) baÅŸarÄ±yla oluÅŸturuldu!")
llm = ChatOpenAI(model_name="gpt-4", temperature=0.2, openai_api_key=openai_api_key)

# --- 5. Alaka Filtresi (LLM as Judge) Zinciri ---
# Bu zincir, bulunan dokÃ¼manlarÄ±n kullanÄ±cÄ± sorusu iÃ§in uygun olup olmadÄ±ÄŸÄ±na karar verir.
relevance_filter_prompt_template = """
KullanÄ±cÄ±nÄ±n Sorusu: {question}

Bulunan DokÃ¼man:
---
{context}
---

GÃ–REV: YukarÄ±daki 'Bulunan DokÃ¼man'Ä±n, 'KullanÄ±cÄ±nÄ±n Sorusu'nu DOÄRUDAN cevaplamak iÃ§in alakalÄ± olup olmadÄ±ÄŸÄ±nÄ± deÄŸerlendir.
CevabÄ±n SADECE 'Evet' veya 'HayÄ±r' olmalÄ±dÄ±r.

AlakalÄ± mÄ±? (Evet/HayÄ±r):
"""
relevance_prompt = PromptTemplate(
    template=relevance_filter_prompt_template,
    input_variables=["question", "context"]
)
relevance_filter_chain = LLMChain(llm=llm, prompt=relevance_prompt)

# --- 6. AkÄ±llÄ± Sorgu ve Cevap DÃ¶ngÃ¼sÃ¼ (YENÄ° MANTIK) ---
print("\nğŸ“˜ Ã–ncelikli Yerel Bilgi AsistanÄ± BaÅŸlatÄ±ldÄ±. Ã‡Ä±kmak iÃ§in 'Ã§Ä±k' yazÄ±n.\n")
while True:
    user_query = input("â“ Soru: ")
    if user_query.lower().strip() in ["Ã§Ä±k", "exit", "quit"]:
        print("ğŸ›‘ Asistan kapatÄ±ldÄ±.")
        break

    try:
        # AdÄ±m 1: Yerel veritabanÄ±nda en benzer dokÃ¼manÄ± bul.
        candidate_docs = db.similarity_search(user_query, k=1)
        
        if not candidate_docs:
            print("\nâš ï¸ Yerel veritabanÄ±nda ilgili hiÃ§bir dokÃ¼man bulunamadÄ±. LLM'e soruluyor...")
            # DoÄŸrudan LLM'e git, Ã§Ã¼nkÃ¼ aday bile yok.
            messages = [
                SystemMessage(content="Sen yardÄ±msever bir asistansÄ±n."),
                HumanMessage(content=user_query)
            ]
            llm_result = llm.invoke(messages)
            print("\nğŸ’¬ Cevap (LLM TarafÄ±ndan Ãœretildi):\n", llm_result.content)
            print("-" * 50)
            continue

        best_doc = candidate_docs[0]

        # AdÄ±m 2: LLM YargÄ±Ã§'a alaka dÃ¼zeyini sor.
        relevance_result = relevance_filter_chain.invoke({
            "question": user_query,
            "context": best_doc.page_content
        })
        is_relevant = "evet" in relevance_result['text'].lower()
        
        print(f"--- YargÄ±Ã§ KararÄ±: Bulunan iÃ§erik alakalÄ± mÄ±? -> {relevance_result['text'].strip()} ---")

        # AdÄ±m 3: YargÄ±Ã§ kararÄ±na gÃ¶re hareket et.
        if is_relevant:
            # ALAKALIYSA: LLM KULLANMA. CevabÄ± doÄŸrudan veritabanÄ±ndan sun.
            print("âœ… Karar: Ä°Ã§erik alakalÄ±. Cevap doÄŸrudan yerel veritabanÄ±ndan sunuluyor...")
            print("\nğŸ’¬ Cevap:\n", best_doc.page_content)
            print("-" * 50)
            print(f"Kaynak: Yerel VeritabanÄ± | Orijinal Soru: {best_doc.metadata.get('original_question', 'N/A')}")
            print("-" * 50)
        else:
            # ALAKALI DEÄÄ°LSE: LLM KULLAN.
            print("\nâŒ Karar: Ä°Ã§erik alakasÄ±z. Cevap LLM ile Ã¼retiliyor...")
            messages = [
                SystemMessage(content="Sen yardÄ±msever bir asistansÄ±n."),
                HumanMessage(content=user_query)
            ]
            llm_result = llm.invoke(messages)
            print("\nğŸ’¬ Cevap (LLM TarafÄ±ndan Ãœretildi):\n", llm_result.content)
            print("-" * 50)
            print("Not: Bu bilgi yerel veritabanÄ±nda bulunamadÄ±ÄŸÄ± iÃ§in LLM tarafÄ±ndan genel bilgiyle cevaplanmÄ±ÅŸtÄ±r.")
            print("-" * 50)

    except Exception as e:
        print(f"âš ï¸ Hata oluÅŸtu: {e}")
